{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#######################\n",
    "def batchgen(batchsize):\n",
    "\n",
    "    def getbatch(x,y):\n",
    "        assert (len(x) == len(y)), \"dimension mismatch\"\n",
    "        for i in range(0, len(y), batchsize):\n",
    "            yield x[i:i+batchsize], y[i:i+batchsize],\n",
    "    return getbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vardict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    def __setattr__(self,name, val):\n",
    "        self[name] = val\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__.items()\n",
    "\n",
    "    def __setstate__(self, items):\n",
    "        for key, val in items:\n",
    "            self.__dict__[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_dict(summary_str, summary_proto = None):\n",
    "    \"convert summary string to a dictionary\"\n",
    "    if summary_proto is None:\n",
    "        summary_proto = tf.Summary()\n",
    "    summary_proto.ParseFromString(summary_str)\n",
    "    summaries = {}\n",
    "    for val in summary_proto.value:\n",
    "        # Assuming all summaries are scalars.\n",
    "        summaries[val.tag] = val.simple_value\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tflearn import tflearn\n",
    "\n",
    "class tflasso(tflearn):\n",
    "    def _create_network(self):\n",
    "        self.vars = vardict()\n",
    "        self.vars.x = tf.placeholder(\"float\", shape=[None, self.xlen])\n",
    "        self.vars.y = tf.placeholder(\"float\", shape=[None, 1])\n",
    "\n",
    "        #def fully_connected():\n",
    "            \n",
    "        # Create Model\n",
    "        self.parameters[\"W1\"] = tf.Variable(tf.truncated_normal([1, self.xlen], stddev=0.1), name=\"weight\")\n",
    "        self.parameters[\"b1\"] = tf.Variable(tf.constant(0.1, shape=[1, 1]), name=\"bias\")\n",
    "        \n",
    "        \n",
    "        self.vars.y_predicted = tf.matmul( self.vars.x, tf.transpose(self.W1)) + self.b1\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def _create_loss(self):\n",
    "        # Minimize the squared errors\n",
    "        l2_loss = tf.reduce_mean(tf.pow( self.vars.y_predicted - self.vars.y, 2))\n",
    "        l2_sy = tf.scalar_summary( \"L2_loss\", l2_loss )\n",
    "        \"Lasso penalty\"\n",
    "        l1_penalty = tf.reduce_sum((tf.abs(tf.concat(1, [self.W1,self.b1], name = \"l1\" ) )) )\n",
    "        l1p_sy =  tf.scalar_summary( \"L1_penalty\" , l1_penalty )\n",
    "        \"total\"\n",
    "        tot_loss = l2_loss + self.ALPHA * l1_penalty\n",
    "        tot_loss_sy =  tf.scalar_summary( \"loss\" , tot_loss )\n",
    "        \"R2\"\n",
    "        _, y_var = tf.nn.moments(self.vars.y, [0,1])\n",
    "        rsq =  1 - l2_loss / y_var\n",
    "        rsq_sy = tf.scalar_summary( \"R2\", rsq)\n",
    "        return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/X (Group) ''\n",
      "  children := ['axis1_label0' (Array), 'axis1_level1' (Array), 'block0_values' (Array), 'axis0_level1' (Array), 'axis1_label1' (Array), 'block0_items_level0' (VLArray), 'axis0_level0' (VLArray), 'block0_items_level1' (Array), 'axis1_level0' (Array), 'axis0_label0' (Array), 'block0_items_label1' (Array), 'block0_items_label0' (Array), 'axis0_label1' (Array)], /y (Group) ''\n",
      "  children := ['index_level1' (Array), 'values' (Array), 'index_level0' (Array), 'index_label0' (Array), 'index_label1' (Array)]]\n"
     ]
    }
   ],
   "source": [
    "datafile = \"../../data/atac_tss_800_1.h5\"\n",
    "\n",
    "with pd.HDFStore(datafile) as store:\n",
    "    print(store.groups())\n",
    "    y_ = store[\"y\"]\n",
    "    X_ = store[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" transform data \"\"\"\n",
    "sys.path.append(\"..\")\n",
    "from transform_tss import safelog, sumstrands, groupcolumns\n",
    "\n",
    "feature_step = 100\n",
    "select = list(feature_step * np.arange(-2,3,1))\n",
    "\n",
    "Xgr = groupcolumns(X_, step = feature_step, select = select)\n",
    "\n",
    "X, y = safelog(Xgr, y_)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pf3 = PolynomialFeatures(degree=3)\n",
    "X3 = pf3.fit_transform(X)\n",
    "trainsamples = 4000\n",
    "train_X, train_Y = X3[:trainsamples], y[:trainsamples].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(pf):\n",
    "    return list(pf._combinations(10, degree=pf.degree,\n",
    "                          interaction_only=pf.interaction_only,\n",
    "                          include_bias = pf.include_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading a session\n",
      "training epochs: 10600 ... 15600, saving each 100' epoch\n",
      "Epoch: 10600\t\n",
      "\ttrain\tR2: 0.3950\tloss: 13.6292\tL2_loss: 10.5626\tL1_penalty: 15.3332\n",
      "Epoch: 10700\t\n",
      "\ttrain\tR2: 0.3951\tloss: 13.6015\tL2_loss: 10.5608\tL1_penalty: 15.2033\n",
      " 66%|██████▌   | 66/100 [00:05<00:02, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.4581682381\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6589692b1f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mALPHA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2e-1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./cubiclasso/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/dima/data/repos/qtl_atac_rna/skl_pipeline/src/tflearn/tflearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_X, train_Y, test_X, test_Y, load)\u001b[0m\n\u001b[0;32m    229\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_x_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_y_\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m                     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmacro_epoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                     \u001b[1;31m# Display logs once in `display_step` epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfl = tflasso(ALPHA = 2e-1, checkpoint_dir = \"./cubiclasso/\", dropout = None)\n",
    "tfl.fit( train_X, train_Y , load = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "load = 0\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tfl._create_network()\n",
    "\n",
    "    tot_loss = tfl._create_loss()\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    sess_config = tf.ConfigProto(inter_op_parallelism_threads=tfl.NUM_CORES,\n",
    "                               intra_op_parallelism_threads= tfl.NUM_CORES)\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session(config = sess_config) as sess:\n",
    "        if load:\n",
    "            tfl._load_(sess)\n",
    "        else:\n",
    "            sess.run(init)\n",
    "\n",
    "        feed_dict={ tfl.vars.x: train_X, }\n",
    "        if tfl.dropout:\n",
    "            feed_dict[ tfl.vars.keep_prob] = 0.5\n",
    "\n",
    "        y_predicted = sess.run( tfl.vars.y_predicted,\n",
    "                        feed_dict = feed_dict )\n",
    "        if y is not None:\n",
    "            feed_dict[ tfl.vars.y ] = np.reshape(y, [-1, 1])\n",
    "            tfl.summary_proto = tf.Summary()\n",
    "            print(tfl.summary_proto.value)\n",
    "            #summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            #summary_d = summary_dict(summary_str, tfl.summary_proto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfl.summary_proto.DESCRIPTOR.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r2_progress not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be a sequence, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5971624c9c1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtfl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr2_progress\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"R^2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: type object argument after * must be a sequence, not NoneType"
     ]
    }
   ],
   "source": [
    "ts, r2s = list(zip( *tfl.r2_progress ))\n",
    "plt.plot(ts, r2s)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.ylim([0, 0.1* np.ceil(10*max(r2s))])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = X3[trainsamples:], y[trainsamples:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading a session\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None of None has invalid type <class 'NoneType'>, must be a string or Tensor. (Can not convert a NoneType into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    321\u001b[0m           fetch_t = self.graph.as_graph_element(subfetch, allow_tensor=True,\n\u001b[1;32m--> 322\u001b[1;33m                                                 allow_operation=True)\n\u001b[0m\u001b[0;32m    323\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[1;32m-> 1960\u001b[1;33m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a NoneType into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-02a78c0cb4ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./cubiclasso/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtfl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mtfl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dima/data/repos/qtl_atac_rna/skl_pipeline/src/tfregression/tflearn.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, load)\u001b[0m\n\u001b[0;32m    160\u001b[0m                                 feed_dict = feed_dict )\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                     \u001b[0msummary_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m                     \u001b[0msummary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0msummary_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    328\u001b[0m           raise TypeError('Fetch argument %r of %r has invalid type %r, '\n\u001b[0;32m    329\u001b[0m                           \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                           % (subfetch, fetch, type(subfetch), str(e)))\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None of None has invalid type <class 'NoneType'>, must be a string or Tensor. (Can not convert a NoneType into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "tfl = tflasso(checkpoint_dir = \"./cubiclasso/\", dropout = False)\n",
    "tfl.transform( test_X, test_Y, load = True)\n",
    "\n",
    "print( tfl.loss )\n",
    "r2 = 1- tfl.loss/test_Y.var()\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf3.powers_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tfl.get_params()[\"W1\"][0]\n",
    "ncoef = len(W1)\n",
    "xlabels = np.array( get_labels(pf3) )\n",
    "\n",
    "forder = np.array([len(x) for x in xlabels])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,figsize = (14, 5))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "for nn in range(3):\n",
    "    valid =( forder == (nn+1))\n",
    "    print(sum(valid))\n",
    "    x_ =  np.arange(ncoef)[valid]\n",
    "    y_ = np.log10( abs(W1[valid]) )\n",
    "    axs[nn].scatter(x_, y_ )\n",
    "    axs[nn].scatter( x_[y_>-3], y_[y_>-3], 25, \"r\" )\n",
    "    #axs[nn].stem( x_[y_>-3], y_[y_>-3], markerfmt = \"ro\" )\n",
    "    if nn < 2:\n",
    "        axs[nn].set_xticks(x_ )\n",
    "        axs[nn].set_xticklabels([repr(x) for x in xlabels[valid]], rotation = 90)\n",
    "    else:\n",
    "        axs[nn].set_xticks(x_[::4] )\n",
    "        axs[nn].set_xticklabels([repr(x) for x in xlabels[valid][::4]], rotation = 90)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.stem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tfl.get_params()[\"W1\"][0]\n",
    "print(len(W1))\n",
    "plt.stem( np.arange(len(W1)), np.log10( abs(W1)) )\n",
    "plt.stem( np.arange(len(W1))[np.log10(W1)>-3], np.log10(W1)[np.log10(W1)>-3], markerfmt = \"ro\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
